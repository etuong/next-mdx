---
title: Machine Learning
metaDesc: My notes on Machine Learning
---

<img src="/images/ml.jpg"/>

In basic terms, ML is the process of training a piece of software, called a model, to make useful predictions using a data set. This predictive model can then serve up predictions about previously unseen data. We use these predictions to take action in a product; for example, the system predicts that a user will like a certain video, so the system recommends that video to the user.

Supervised learning is a type of ML where the model is provided with labeled training data. Given an input feature (X), you are telling the system what the expected output label (Y) is.

Examples of labels: Future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip,
Examples of features: Words in the email text, number of black pixels

In unsupervised learning, the goal is to identify meaningful patterns in the data. To accomplish this, the machine must learn from an unlabeled data set. In other words, the model has no hints how to categorize each piece of data and must infer its own rules for doing so.

ML is like natural sciences.

*Steps*
1. Set the research goal
1. Make a hypothesis
1. Collect the data
1. Test your hypothesis
1. Analyze your results
1. Reach a conclusion
1. Refine hypothesis and repeat

*Examples*
1. I want to predict how heavy traffic will be on a given day
1. I think the weather forecast is an informative signal 
1. Collect historical traffic data and weather on each day 
1. Train a model using this data 
1. Is this model better than existing systems? 
1. I should (not) use this model to make predictions, because of X, Y, and Z 
1. Time of year could be a helpful signal 

Classification is discrete - yes or no, true or false etc. The dependent variable outcome is discrete. Questions it answers include "Is a given email message spam or not spam?" and "Is this an image of a dog, a cat, or a hamster?"

Regression is continuous model to predict the next label.
Some examples of classification include Decision Tree, K-Nearest Neighbor (determine what the given object is based on its similarity to other comparable objects), Logistc Regression (a dataset with one or more independent variables is used to determine binary output of the dependent variable).

Linear regression is used to solve regression problems and continous in nature. Shape curve is linear.
Logistc regression is used to solve classification problems and is categorical in nature. Curve shape is sigmoid.

Machine learning is the science of making computers learn and act like humans by feeding data and information without being explicitly programmed.

Steps to machine learning: define objective, collect data, prepare data, select algorithm, train model, test model, predict, deploy.

Do you want to predict a category? If so that's a predict. Classication problems are yes or no. Do you want to predict a quantity? That's regression, for example predicting the age of a person based on height, weight, health, and other factors. Do you want to discover structure in unexplored data? That's clustering, for example finding groups of customers with similar behavior given a large database of customer data containing their demographics and past buying records.

Supervised learning is a method used to enable machines to classify/predict objects, problems or situations based on labeled data fed to the machine.

Reinforcement learning is an important type of Machine Learning where an agent learns how to behave in an environment by performing actions and seeing the results.

A decision tree is a literal tree of decisions where nodes branches from data (or conditions). Entropy is the measure of randonmess or "impurity" in the dataset. We want entropy to be low. Information Gain is the measure in entropy after the dataset is split. Information Gain should be high. We choose the attribute with the largest information gain as the root node.

Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. It's basically the offset between the prediction and true features.

It will be nice to have some sort of function that quantifies the losses. A popular loss function is the error square which is defined as the square of the difference between the label and the prediction.

The loss versus weights plot is always convex (happy face shape). Our goal is to find the weight where the deriative is zero. There is only one place where the slope is exactly zero and to calculate the loss function for every conceivable value of w's over the entire data set is inefficient to find this convergence point. A popular technique is gradient descent.

Gradient vector has both a direction and a magnitude. To find the next point on the loss curve, we multiple the gradient by a scalar known as the learning rate (aka step size). This is an iterative process. Machine learning programmers will play with knobs called hyperparameters to fine tune the learning rate. A small learning rate will take too long to learn, a big learning rate will overshoot the minimum.

Goldilock is the learning curve with the fewest number of steps required to reach the minimum.

A large set of data in magnitude of billions may contain redundant data which in turn will cause a long computation for the gradient descent. To work around this, we sample examples randomly in a technique called Stochastic gradient descent (SGD).

Common hyperparameters in machine learning include "steps" which is the total number of training iterations. One step calculates the loss from one batch and uses the value to modify the model's weight once. "Batch size" which is the number of examples (chosen at random) for a single step. Total number of trained examples = batch size \* steps.

Train the model with a training set and validate it with a test set. Training set could be 80% and test set can be 20% or the data set. Make sure test set doesn't contain duplicates from training set.

Overfitting is tempting to satisfy one or more examples but simplicity is the way to go.

A validation set is a partition of the training set to prevent overfitting on peculiar examples.

Feature engineering is the process of creating features from raw data.

Mapping numeric values is straightforward but mapping strings like street address isn't. One approach is to use a mapping technique called one-hot encoding.

Good feature values should appear more than 5 or so times in a data set e.g house_id will not be a good feature.

Cleaning data throws away the bad data. (1) Scaling helps gradient descent to converge more quickly because the range of focus is more narrow. (2) Handle extreme outliers. Using a log scale graph helps with this visual. (3) Other techniques include binning and scrubbing.
